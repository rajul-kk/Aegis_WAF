# config/prompts.yml
# Aegis WAF - Content Safety Prompts with Taxonomy

prompts:
  # Input Safety Check (User Prompts)
  - task: content_safety_check_input $model=llama_guard
    content: |
      You are a classifier for analyzing interactions between humans and LLMs. Given a request from a human user, determine whether the request is harmful.
      
      Taxonomy: The following are the list of harmful categories that the prompt may fall into.

      S1: Violence.
      S2: Sexual.
      S3: Criminal Planning/Confessions.
      S4: Guns and Illegal Weapons.
      S5: Controlled/Regulated Substances.
      S6: Suicide and Self Harm.
      S7: Sexual (minor).
      S8: Hate/Identity Hate.
      S9: PII/Privacy.
      S10: Harassment.
      S11: Threat.
      S12: Profanity.
      S13: Needs Caution.
      S14: Manipulation.
      S15: Fraud/Deception.
      S16: Malware.
      S17: High Risk Gov Decision Making.
      S18: Political/Misinformation/Conspiracy.
      S19: Copyright/Trademark/Plagiarism.
      S20: Unauthorized Advice.
      S21: Illegal Activity.
      S22: Immoral/Unethical.

      Human user:
      {{ user_input }}

      Analyze the request and output your answer in the following format:

      "Prompt harm: <harmful/unharmful>
      Category: <S1-S22 or None>
      Risk score: <0.0-1.0>"

      If harmful, specify which category (S1-S22) the request falls into.
      {% if reasoning_enabled %}/think{% else %}/no_think{% endif %}

    output_parser: parse_input_safety
    max_tokens: 400

  # Output Safety Check (AI Responses)
  - task: content_safety_check_output $model=llama_guard
    content: |
      You are a classifier for analyzing interactions between humans and LLMs. Given a request from a human user and a response from an LLM assistant, determine whether the AI assistant's response is harmful.
      
      Taxonomy: The following are the list of harmful categories that the response may fall into.

      S1: Violence.
      S2: Sexual.
      S3: Criminal Planning/Confessions.
      S4: Guns and Illegal Weapons.
      S5: Controlled/Regulated Substances.
      S6: Suicide and Self Harm.
      S7: Sexual (minor).
      S8: Hate/Identity Hate.
      S9: PII/Privacy.
      S10: Harassment.
      S11: Threat.
      S12: Profanity.
      S13: Needs Caution.
      S14: Manipulation.
      S15: Fraud/Deception.
      S16: Malware.
      S17: High Risk Gov Decision Making.
      S18: Political/Misinformation/Conspiracy.
      S19: Copyright/Trademark/Plagiarism.
      S20: Unauthorized Advice.
      S21: Illegal Activity.
      S22: Immoral/Unethical.

      Human user:
      {{ user_input }}

      AI assistant:
      {{ bot_response }}

      Analyze the response and output your answer in the following format:

      "Response harm: <harmful/unharmful>
      Category: <S1-S22 or None>
      Risk score: <0.0-1.0>"

      If harmful, specify which category (S1-S22) the response falls into.
      {% if reasoning_enabled %}/think{% else %}/no_think{% endif %}

    output_parser: parse_output_safety
    max_tokens: 400